{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Tree Pipeline for Jet Engine Fault Detection\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [19, 8]\n",
        "\n",
        "DATA_PATH = 'engine_fault_detection_dataset.csv'\n",
        "TARGET_COL = 'Engine_Condition'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "engine_df = pd.read_csv(DATA_PATH)\n",
        "print(\"Loaded:\", engine_df.shape)\n",
        "engine_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA: info, description, nulls\n",
        "print(engine_df.info())\n",
        "print(engine_df.describe().T)\n",
        "print(\"\\nNull counts:\\n\", engine_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA: class distribution\n",
        "sns.countplot(data=engine_df, x=TARGET_COL)\n",
        "plt.title(\"Class Distribution\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA: correlations heatmap for numeric\n",
        "num_df = engine_df.select_dtypes(include=np.number)\n",
        "sns.heatmap(num_df.corr(), annot=False, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess: split, scaling optional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = engine_df.drop(columns=[TARGET_COL])\n",
        "y = engine_df[TARGET_COL]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "  X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "numeric_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "preprocessor = ColumnTransformer([\n",
        "  (\"num\", StandardScaler(), numeric_features)\n",
        "], remainder='drop')\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decision Tree training and evaluation (fast)\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Using class_weight to counter imbalance (DT supports it via sample weighting workaround not native);\n",
        "# we'll provide sample_weight during fit if needed. Here we use min-cost-complexity pruning.\n",
        "\n",
        "dt = DecisionTreeClassifier(\n",
        "  criterion='gini',\n",
        "  splitter='best',\n",
        "  max_depth=None,\n",
        "  min_samples_split=4,\n",
        "  min_samples_leaf=2,\n",
        "  ccp_alpha=0.0,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "dt_pipeline = Pipeline([\n",
        "  (\"preprocess\", preprocessor),\n",
        "  (\"model\", dt)\n",
        "])\n",
        "\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "dt_pred = dt_pipeline.predict(X_test)\n",
        "print(\"DT Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"DT Classification Report:\\n\", classification_report(y_test, dt_pred))\n",
        "print(\"DT Confusion Matrix:\\n\", confusion_matrix(y_test, dt_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation and light tuning for Decision Tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "  'model__max_depth': [None, 6, 10, 16, 24],\n",
        "  'model__min_samples_split': [2, 4, 8, 16],\n",
        "  'model__min_samples_leaf': [1, 2, 4, 8],\n",
        "  'model__ccp_alpha': [0.0, 0.0005, 0.001, 0.005]\n",
        "}\n",
        "\n",
        "gs = GridSearchCV(\n",
        "  estimator=dt_pipeline,\n",
        "  param_grid=param_grid,\n",
        "  scoring='f1_macro',\n",
        "  cv=5,\n",
        "  n_jobs=-1,\n",
        "  verbose=1\n",
        ")\n",
        "\n",
        "gs.fit(X_train, y_train)\n",
        "print(\"Best DT params:\", gs.best_params_)\n",
        "\n",
        "dt_best = gs.best_estimator_\n",
        "dt_best_pred = dt_best.predict(X_test)\n",
        "print(\"DT Tuned Accuracy:\", accuracy_score(y_test, dt_best_pred))\n",
        "print(\"DT Tuned Classification Report:\\n\", classification_report(y_test, dt_best_pred))\n",
        "print(\"DT Tuned Confusion Matrix:\\n\", confusion_matrix(y_test, dt_best_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save DT model and feature metadata\n",
        "import joblib\n",
        "artifacts = Path('artifacts_dt')\n",
        "artifacts.mkdir(exist_ok=True)\n",
        "\n",
        "joblib.dump(dt_best, artifacts / 'jet_fault_dt_model.pkl')\n",
        "metadata = {\n",
        "  'numeric_features': numeric_features,\n",
        "  'target': TARGET_COL,\n",
        "  'classes_': sorted(y.unique())\n",
        "}\n",
        "joblib.dump(metadata, artifacts / 'feature_metadata.pkl')\n",
        "print('Saved Decision Tree model to artifacts_dt/jet_fault_dt_model.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
